- name: Clone VMs from existing templates
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Clone template to VM-LB-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-LB-01
        newid: 200
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Clone template to VM-SWARM-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-SWARM-01
        newid: 201
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Clone template to VM-SWARM-02
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-SWARM-02
        newid: 202
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Clone template to VM-SWARM-03
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-SWARM-03
        newid: 203
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Clone template to VM-GIT-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-GIT-01
        newid: 204
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Clone template to VM-K3S-01-MASTER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-K3S-01-MASTER
        newid: 205
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10
    
    - name: Clone template to VM-K3S-02-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-K3S-02-WORKER
        newid: 206
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Clone template to VM-K3S-03-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-K3S-03-WORKER
        newid: 207
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10



- name: Update VM configurations with correct hardware assignment and cloud-init variables
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Edit VM-LB-01 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-LB-01
        tags: load-balancer,docker,glusterfs
        cores: 4
        memory: 2048
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.160/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Edit VM-SWARM-01 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-01
        tags: docker,app-server,glusterfs
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.161/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Edit VM-SWARM-02 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-02
        tags: docker,app-server,glusterfs
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.162/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Edit VM-SWARM-03 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-03
        tags: docker,app-server,glusterfs
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.163/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Edit VM-GIT-01 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-GIT-01
        tags: gitlab,app-server
        cores: 4
        memory: 2048
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.164/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10
    
    - name: Edit VM-K3S-01-MASTER with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-01-MASTER
        tags: kubernetes,app-server,master
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.165/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Edit VM-K3S-02-WORKER with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-02-WORKER
        tags: kubernetes,app-server,worker
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.166/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Edit VM-K3S-03-WORKER with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-03-WORKER
        tags: kubernetes,app-server,worker
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.167/24
        update: true
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10



- name: Resize VM disks to the required size
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Re-size VM-LB-01 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-LB-01
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-SWARM-01 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-01
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-SWARM-02 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-02
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-SWARM-03 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-03
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-GIT-01 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-GIT-01
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-K3S-01-MASTER disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-01-MASTER
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-K3S-02-WORKER disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-02-WORKER
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10

    - name: Re-size VM-K3S-03-WORKER disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-03-WORKER
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10



- name: Start virtual machines
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Start VM-LB-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-LB-01
        state: started
        timeout: 300

    - name: Start VM-SWARM-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-01
        state: started
        timeout: 300

    - name: Start VM-SWARM-02
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-02
        state: started
        timeout: 300

    - name: Start VM-SWARM-03
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-03
        state: started
        timeout: 300

    - name: Start VM-GIT-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-GIT-01
        state: started
        timeout: 300
    
    - name: Start VM-K3S-01-MASTER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-01-MASTER
        state: started
        timeout: 300

    - name: Start VM-K3S-02-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-02-WORKER
        state: started
        timeout: 300

    - name: Start VM-K3S-03-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-03-WORKER
        state: started
        timeout: 300

    - name: Pause for 5 minutes
      ansible.builtin.pause:
        seconds: 300



- name: Reboot all VMs to ensure that processes are cleared from install
  hosts: docker_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Reboot systems [Linux]
      reboot:
        post_reboot_delay: 60
        test_command: uptime



- name: Install updates on newly created VMs
  hosts: docker_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Preliminary checks [Debian/Ubuntu]
      file:
        path: /var/lib/apt/lists/
        state: directory
        mode: 0755
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install all updates [Debian/Ubuntu]
      apt:
        update_cache: yes
        upgrade: dist
        autoclean: yes
      when:
        - ansible_facts.os_family == "Debian"

    - name: Upgrade all packages and install kernel updates [CentOS/Rocky]
      dnf:
        update_cache: yes
        update_only: yes
        name: '*'
        state: latest
      when:
        - ansible_facts.os_family == "RedHat"



- name: Install base packages and requirements, setup cockpit with plugins and ensure qemu-guest-agent is working
  hosts: docker_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Install required packages (qemu-guest-agent, cockpit, cifs-utils, nfs-common, python3, pip, neofetch, glusterfs) [Debian/Ubuntu]
      apt:
        name:
        - qemu-guest-agent
        - cockpit
        - cifs-utils
        - nfs-common
        - keyutils
        - snmpd
        - snmp
        - libsnmp-dev
        - software-properties-common
        - apt-transport-https
        - ca-certificates
        - curl
        - python3-pip
        - virtualenv
        - python3-setuptools
        - neofetch
        - glusterfs-server
        - glusterfs-client
        state: present
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install required base-packages (qemu-guest-agent, cifs-utils, nfs-common, python3, pip, neofetch, glusterfs) [CentOS/Rocky]
      dnf:
        name: 
        - qemu-guest-agent
        - cifs-utils
        - nfs-common
        - keyutils
        - epel-release
        - net-snmp
        - net-snmp-utils
        - software-properties-common
        - apt-transport-https
        - ca-certificates
        - curl
        - python3-pip
        - virtualenv
        - python3-setuptools
        - neofetch
        - glusterfs-server
        - glusterfs-client
        - policycoreutils
        state: present
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Install cockpit navigator plugin [Debian/Ubuntu]
      apt:
        deb: https://github.com/45Drives/cockpit-navigator/releases/download/v0.5.10/cockpit-navigator_0.5.10-1focal_all.deb
        state: present
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install cockpit navigator plugin [CentOS/Rocky]
      dnf:
        name: https://github.com/45Drives/cockpit-navigator/releases/download/v0.5.8/cockpit-navigator-0.5.8-2.el8.noarch.rpm
        state: present
        disable_gpg_check: True
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Install cockpit file sharing plugin [Debian/Ubuntu]
      apt:
        deb: https://github.com/45Drives/cockpit-file-sharing/releases/download/v3.3.2/cockpit-file-sharing_3.3.2-1focal_all.deb
        state: present
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install cockpit file sharing plugin [CentOS/Rocky]
      dnf:
        name: https://github.com/45Drives/cockpit-file-sharing/releases/download/v2.4.5/cockpit-file-sharing-2.4.5-1.el8.noarch.rpm
        state: present
        disable_gpg_check: True
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Start qemu-guest-agent service
      systemd:
        name: qemu-guest-agent
        enabled: yes
        state: started

    - name: Create file with NetworkManager config to fix issue with NetworkManager and Cockpit [Debian/Ubuntu]
      copy:
        dest: /etc/NetworkManager/conf.d/10-globally-managed-devices.conf
        content: |
          [keyfile]
          unmanaged-devices=none
      when:
        - ansible_facts.os_family == "Debian"

    - name: Create fake network interface to fix issue with NetworkManager and Cockpit [Debian/Ubuntu]
      ansible.builtin.command: nmcli con add type dummy con-name fake ifname fake0 ip4 1.2.3.4/24 gw4 1.2.3.1
      when:
        - ansible_facts.os_family == "Debian"

    - name: Start cockpit service [Debian/Ubuntu]
      systemd:
        name: cockpit
        enabled: yes
        state: started
        masked: no
      when:
        - ansible_facts.os_family == "Debian"

    - name: Enable cockpit.socket [CentOS/Rocky]
      systemd:
        name: cockpit.socket
        enabled: yes
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Start cockpit [CentOS/Rocky]
      systemd:
        name: cockpit
        state: started
      when:
        - ansible_facts.os_family == "RedHat"



- name: Configure SNMP service and test to ensure it reports correctly
  hosts: docker_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Install pysnmp for testing SNMP on hosts
      pip:
        name: pysnmp

    - name: Configure SNMP server
      copy: src=/home/abeckett/snmpd.conf dest=/etc/snmp/snmpd.conf owner=root group=root mode=600

    - name: Start and enable SNMPd systemd service.
      systemd:
        name: snmpd
        state: started
        enabled: true
        daemon_reload: true

    - name: Restart SNMPd systemd service if already running.
      systemd:
        name: snmpd
        state: restarted

    - name: Test that SNMP is working
      community.general.snmp_facts:
        host: '{{ inventory_hostname }}'
        version: v2c
        community: BCloudHomelab
      register: response

    - name: Print the results of the test
      debug:
        var: response



- name: Create mountpoint and mount network shares
  hosts: docker_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Create NAS-01 local directory
      ansible.builtin.file:
        path: /nas-01
        state: directory

    - name: Mount NAS-01 network share
      ansible.posix.mount:
        src: NAS-01.homelab.internal:/volume1/Media
        path: /nas-01
        fstype: nfs
        boot: true
        opts: rw,sync,hard
        state: mounted

    - name: Create STORAGE-01 local directory
      ansible.builtin.file:
        path: /storage-01
        state: directory

    - name: Mount STORAGE-01 network share
      ansible.posix.mount:
        src: STORAGE-01.homelab.internal:/mnt/user/media
        path: /storage-01
        fstype: nfs
        boot: true
        opts: rw,sync,hard
        state: mounted



- name: Snapshot all VMs after initial configuration before applications and services are installed and started
  hosts: proxmox_servers
  become: true
  tasks:  
    - name: Take a snapshot of VM-LB-01 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 200
        snapname: Post-update-early-configuration
        state: present

    - name: Take a snapshot of VM-SWARM-01 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 201
        snapname: Post-update-early-configuration
        state: present
    
    - name: Take a snapshot of VM-SWARM-02 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 202
        snapname: Post-update-early-configuration
        state: present
    
    - name: Take a snapshot of VM-SWARM-03 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 203
        snapname: Post-update-early-configuration
        state: present
    
    - name: Take a snapshot of VM-GIT-01 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 204
        snapname: Post-update-early-configuration
        state: present
    
    - name: Take a snapshot of VM-K3S-01-MASTER with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 205
        snapname: Post-update-early-configuration
        state: present
    
    - name: Take a snapshot of VM-K3S-02-WORKER with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 206
        snapname: Post-update-early-configuration
        state: present

    - name: Take a snapshot of VM-K3S-03-WORKER with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 207
        snapname: Post-update-early-configuration
        state: present



- name: Install glusterFS/glusterd and create directory for brick
  hosts: docker_vms
  become: true
  tasks:
    - name: Start glusterd service
      systemd:
        name: glusterd
        enabled: yes
        state: started

    - name: Create GlusterFS brick-1 directory
      ansible.builtin.file:
        path: /glusterfs/bricks/gv0
        state: directory



- name: Install docker and docker python module on docker VMs
  hosts: docker_vms
  become: true
  tasks:    
    - name: Add Docker GPG Key [Debian/Ubuntu]
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
      when:
        - ansible_facts.os_family == "Debian"

    - name: Add Docker Repository [Debian/Ubuntu]
      apt_repository:
        repo: deb https://download.docker.com/linux/ubuntu focal stable
        state: present
      when:
        - ansible_facts.os_family == "Debian"

    - name: Add Docker Repository [CentOS/Rocky]
      shell: |
        dnf install -y yum-utils
        yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
      args:
        warn: no
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Install the latest version of Docker Engine and containerd [Debian/Ubuntu]
      apt:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
        state: latest
        update_cache: true
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install the latest version of Docker Engine and containerd [CentOS/Rocky]
      dnf:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
        state: latest
        update_cache: true
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Add the admin user to the docker group
      user:
        name: "{{ username }}"
        groups: docker
        append: yes

    - name: Install Docker Module for Python
      pip:
        name: docker

    - name: Start and enable Docker systemd service.
      systemd:
        name: docker
        state: started
        enabled: true
        daemon_reload: true



- name: Create mountpoint and glusterFS volume, and mount to host machine
  hosts: loadbalancer
  become: true
  tasks:
    - name: Create a trusted storage pool
      gluster.gluster.gluster_peer:
        state: present
        nodes:
          - 192.168.0.161
          - 192.168.0.162
          - 192.168.0.163

    - name: Create GlusterFS volume
      gluster.gluster.gluster_volume:
        state: present
        name: gv0
        replicas: 4
        bricks: /glusterfs/bricks/gv0
        cluster:
          - 192.168.0.160
          - 192.168.0.161
          - 192.168.0.162
          - 192.168.0.163
        force: true
      run_once: true

    - name: Start GlusterFS volume
      gluster.gluster.gluster_volume:
        state: started
        name: gv0

    - name: Create mountpoint for Gluster volume
      ansible.builtin.file:
        path: /mnt/gluster
        state: directory

    - name: Mount Gluster volume
      ansible.posix.mount:
        src: "{{ ansible_host }}:/gv0"
        path: /mnt/gluster
        fstype: glusterfs
        state: mounted



- name: Deploy docker containers for portainer, NGINX and file manager on the loadbalancer node
  hosts: loadbalancer
  become: true
  tasks:
    - name: Spin-up a Portainer instance for management of Swarm
      docker_container:
        name: Portainer
        image: portainer/portainer-ce
        state: started
        recreate: yes
        restart_policy: always
        published_ports:
          - "8000:8000"
          - "9000:9000"
          - "9443:9443"
        volumes:
          - /var/run/docker.sock:/var/run/docker.sock
          - portainer_data:/data

    - name: Deploy NGINX load-balancer
      community.docker.docker_container:
        name: Load-Balancer
        image: nginx
        state: started
        volumes:
          - /nas-01/Container-configs/loadbalancer/:/etc/nginx/conf.d
        ports:
        - 9003-9020:9003-9020
        - 10000-10020:10000-10020
        - 11000-11020:11000-11020
        - 12000-12020:12000-12020
        - 13000-13020:13000-13020
        - 14000-14020:14000-14020
        - 15000-15020:15000-15020
        restart_policy: always

    - name: Deploy File-Manager for GlusterFS volume
      community.docker.docker_container:
        name: File-Manager
        image: hurlenko/filebrowser
        state: started
        volumes:
          - /nas-01/Container-configs/file-browser-glusterfs:/config
          - /mnt/gluster:/data
        ports:
          - 9002:8080
        env:
          PUID: "1000"
          PGID: "1000"
          TZ: "Europe/London"
        restart_policy: always



- name: Create mountpoint and mount glusterFS volume
  hosts: swarm_manager
  become: true
  tasks:
    - name: Create mountpoint for Gluster volume
      ansible.builtin.file:
        path: /mnt/gluster
        state: directory

    - name: Mount Gluster volume
      ansible.posix.mount:
        src: "{{ ansible_host }}:/gv0"
        path: /mnt/gluster
        fstype: glusterfs
        state: mounted



- name: Initialize docker swarm and grab the join token
  hosts: swarm_manager
  become: true
  tasks:
    - name: Check if Swarm has already been Initialized
      shell: docker node ls
      register: swarm_status
      ignore_errors: true

    - name: Initialize Docker Swarm
      docker_swarm:
        state: present
      when: swarm_status.rc != 0
      run_once: true

    - name: Change task-history limit to 0
      shell: docker swarm update --task-history-limit 0

    - name: Get the worker join-token
      shell: docker swarm join-token --quiet worker
      register: worker_token



- name: Add nodes to the docker swarm
  hosts: swarm_nodes
  become: true
  tasks:
    - name: Add Workers to the Swarm
      community.docker.docker_swarm:
        state: join
        advertise_addr: "{{ ansible_host }}"
        join_token: "{{ hostvars[groups['swarm_manager'][0]]['worker_token']['stdout'] }}"
        remote_addrs: [ "{{ hostvars[groups['swarm_manager'][0]]['ansible_default_ipv4']['address'] }}:2377" ]



- name: Create mountpoint and mount glusterFS volume
  hosts: swarm_nodes
  become: true
  tasks:
    - name: Create mountpoint for Gluster volume
      ansible.builtin.file:
        path: /mnt/gluster
        state: directory

    - name: Mount Gluster volume
      ansible.posix.mount:
        src: "{{ ansible_host }}:/gv0"
        path: /mnt/gluster
        fstype: glusterfs
        state: mounted



- name: Create the portainer agent network and deploy the portainer agent
  hosts: swarm_manager
  become: true
  tasks:
    - name: Create the portainer_agent_network
      docker_network:
        name: portainer_agent_network
        driver: overlay

    - name: Start portainer_agent service
      docker_swarm_service:
        name: portainer_agent
        image: portainer/agent:2.18.2
        networks:
          - portainer_agent_network
        publish:
          - published_port: 9001
            target_port: 9001
            protocol: tcp
        mode: global
        placement:
          constraints:
            - node.platform.os == linux
        mounts:
          - source: //var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
          - source: //var/lib/docker/volumes
            target: /var/lib/docker/volumes
            type: bind



- name: Setup repository and install Gitlab
  hosts: git
  become: true
  tasks:
    - name: Add Gitlab Repository [Debian/Ubuntu]
      shell: curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash
      when:
        - ansible_facts.os_family == "Debian"

    - name: Add Gitlab Repository [CentOS/Rocky]
      shell: curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh | sudo bash
      when:
        - ansible_facts.os_family == "RedHat"

    - name: Pause for 60 seconds
      ansible.builtin.pause:
        seconds: 60

    - name: Install Gitlab [Debian/Ubuntu]
      apt:
        name: gitlab-ee
        state: present
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install Gitlab [CentOS/Rocky]
      dnf:
        name: gitlab-ee
        state: present
      when:
        - ansible_facts.os_family == "RedHat"
    
    - name: Pause for 5 minutes
      ansible.builtin.pause:
        seconds: 300

    - name: Change External URL in Gitlab config file
      ansible.builtin.lineinfile:
        path: /etc/gitlab/gitlab.rb
        regexp: '^external_url.*$'
        line: 'external_url "http://gitlab.beckettcloud.com"'
    
    - name: Restart Gitlab to update the changes
      shell: gitlab-ctl restart



- name: Download and configure K3S on the VMs
  hosts: k3s_vms
  become: true
  tasks:
    - name: Install dependencies
      pip:
        name: 
          - pyyaml
          - jsonpatch
          - kubernetes

    - name: Download k3s binary x64
      get_url:
        url: https://github.com/k3s-io/k3s/releases/download/v1.25.11+k3s1/k3s
        checksum: sha256:https://github.com/k3s-io/k3s/releases/download/v1.25.11+k3s1/sha256sum-amd64.txt
        dest: /usr/local/bin/k3s
        owner: root
        group: root
        mode: 0755

    - name: Enable IPv4 forwarding
      sysctl:
        name: net.ipv4.ip_forward
        value: "1"
        state: present
        reload: yes

    - name: Enable IPv6 forwarding
      sysctl:
        name: net.ipv6.conf.all.forwarding
        value: "1"
        state: present
        reload: yes
    
    - name: Add br_netfilter to /etc/modules-load.d/ [CentOS/Rocky]
      copy:
        content: "br_netfilter"
        dest: /etc/modules-load.d/br_netfilter.conf
        mode: "u=rw,g=,o="
      when: ansible_facts.os_family == "RedHat"

    - name: Load br_netfilter [CentOS/Rocky]
      modprobe:
        name: br_netfilter
        state: present
      when: ansible_facts.os_family == "RedHat"

    - name: Set bridge-nf-call-iptables (just to be sure) [CentOS/Rocky]
      sysctl:
        name: "{{ item }}"
        value: "1"
        state: present
        reload: yes
      when: ansible_facts.os_family == "RedHat"
      loop:
        - net.bridge.bridge-nf-call-iptables
        - net.bridge.bridge-nf-call-ip6tables

    - name: Add /usr/local/bin to sudo secure_path [CentOS/Rocky]
      lineinfile:
        line: 'Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin'
        regexp: "Defaults(\\s)*secure_path(\\s)*="
        state: present
        insertafter: EOF
        path: /etc/sudoers
        validate: 'visudo -cf %s'
      when: ansible_facts.os_family == "RedHat"



- name: Deploy the K3S service on the master node and grab the join token for worker nodes
  hosts: k3s_master
  become: true
  tasks:
    - name: Copy K3s service file
      register: k3s_service
      template:
        src: "/home/abeckett/k3s.service.j2"
        dest: "/etc/systemd/system/k3s.service"
        remote_src: no
        owner: root
        group: root
        mode: 0644

    - name: Enable and check K3s service
      systemd:
        name: k3s
        daemon_reload: yes
        state: restarted
        enabled: yes

    - name: Wait for node-token
      wait_for:
        path: "/var/lib/rancher/k3s/server/node-token"

    - name: Register node-token file access mode
      stat:
        path: "/var/lib/rancher/k3s/server/node-token"
      register: p

    - name: Change file access node-token
      file:
        path: "/var/lib/rancher/k3s/server/node-token"
        mode: "g+rx,o+rx"

    - name: Read node-token from master
      slurp:
        path: "/var/lib/rancher/k3s/server/node-token"
      register: node_token

    - name: Store Master node-token
      set_fact:
        token: "{{ node_token.content | b64decode | regex_replace('\n', '') }}"

    - name: Restore node-token file access
      file:
        path: "/var/lib/rancher/k3s/server/node-token"
        mode: "{{ p.stat.mode }}"
    
    - name: Create directory .kube
      file:
        path: ~abeckett/.kube
        state: directory
        owner: "abeckett"
        mode: "u=rwx,g=rx,o="

    - name: Copy config file to user home directory
      copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: ~abeckett/.kube/config
        remote_src: yes
        owner: abeckett
        mode: '600'

    - name: Export Kubeconfig
      shell: |
            export KUBECONFIG=~/.kube/config

    - name: Replace https://localhost:6443 by https://192.168.0.165:6443
      command: >-
        k3s kubectl config set-cluster default --server=https://192.168.0.165:6443 --kubeconfig ~abeckett/.kube/config
      changed_when: true

    - name: Create kubectl symlink
      file:
        src: /usr/local/bin/k3s
        dest: /usr/local/bin/kubectl
        state: link

    - name: Create crictl symlink
      file:
        src: /usr/local/bin/k3s
        dest: /usr/local/bin/crictl
        state: link

    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10



- name: Deploy the K3S service on worker nodes and join to the cluster
  hosts: k3s_nodes
  become: true
  tasks:
    - name: Copy K3s service file
      template:
        src: "/home/abeckett/k3s-node.service.j2"
        dest: "/etc/systemd/system/k3s-node.service"
        remote_src: no
        owner: root
        group: root
        mode: 0755

    - name: Enable and check K3s service
      systemd:
        name: k3s-node
        daemon_reload: yes
        state: restarted
        enabled: yes

    - name: Pause for 10 seconds
      ansible.builtin.pause:
        seconds: 10



- name: Deploy Rancher
  hosts: k3s_master
  become: true
  tasks:
    - name: Add Helm install repository
      shell: |
            curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list

    - name: Install Helm [Debian/Ubuntu]
      apt:
        name:
        - helm
        state: present
        update_cache: yes
      when:
        - ansible_facts.os_family == "Debian"

    - name: Install Helm [CentOS/Rocky]
      dnf:
        name:
        - helm
        state: present
        update_cache: yes
      when:
        - ansible_facts.os_family == "RedHat"
    
    - name: Add the Helm repository for Rancher and dependencies
      kubernetes.core.helm_repository:
        name: rancher-latest
        repo_url: https://releases.rancher.com/server-charts/latest

    - name: Download Cert-Manager
      shell: |
            kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.4/cert-manager.crds.yaml

    - name: Add Jetstack repository
      kubernetes.core.helm_repository:
        name: jetstack
        repo_url: https://charts.jetstack.io

    #- name: Add Jetstack repository
    #  shell: |
    #        helm repo add jetstack https://charts.jetstack.io
            
    #- name: Update helm repositories
    #  shell: |
    #        helm repo update

    - name: Deploy latest version of Cert-Manager chart inside cert-manager namespace (and create it)
      kubernetes.core.helm:
        update_repo_cache: true
        name: cert-manager
        chart_ref: jetstack/cert-manager
        chart_version: v1.11.4
        release_namespace: cert-manager
        create_namespace: true

    #- name: Install Cert-Manager
    #  command: helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.11.0

    - name: Deploy latest version of Rancher chart inside cattle-system namespace (and create it)
      kubernetes.core.helm:
        update_repo_cache: true
        name: rancher
        chart_ref: rancher-latest/rancher
        release_namespace: cattle-system
        values:
          - hostname: 192.168.0.165
          - bootstrapPassword: "{{ password }}"
        #set_values:
        #  - value: hostname=192.168.0.165
        #  - value_type: string
        #  - value: replicas=1
        #  - value_type: string
        #  - value: bootstrapPassword="{{ password }}"
        #  - value_type: string


    #- name: Add the Helm repository for Rancher and dependencies
    #  command: helm install rancher rancher-latest/rancher --namespace cattle-system --set hostname=192.168.0.165 --set replicas=1 --set bootstrapPassword="{{ password }}"
    


- name: Reboot all VMs to finish updates and ensure all services run correctly
  hosts: docker_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Reboot systems [Linux]
      reboot:
        post_reboot_delay: 60
        test_command: uptime



- name: Snapshot all VMs after all configuration has been completed and they are ready for active deployment
  hosts: proxmox_servers
  become: true
  tasks:  
    - name: Take a snapshot of VM-LB-01 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 200
        snapname: Post-configuration
        state: present

    - name: Take a snapshot of VM-SWARM-01 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 201
        snapname: Post-configuration
        state: present
    
    - name: Take a snapshot of VM-SWARM-02 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 202
        snapname: Post-configuration
        state: present
    
    - name: Take a snapshot of VM-SWARM-03 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 203
        snapname: Post-configuration
        state: present
    
    - name: Take a snapshot of VM-GIT-01 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 204
        snapname: Post-configuration
        state: present

    - name: Take a snapshot of VM-K8S-01-MASTER with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 205
        snapname: Post-configuration
        state: present
    
    - name: Take a snapshot of VM-K8S-02-WORKER with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 206
        snapname: Post-configuration
        state: present

    - name: Take a snapshot of VM-K8S-03-WORKER with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 207
        snapname: Post-configuration
        state: present