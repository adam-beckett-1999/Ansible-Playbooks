#CREATE, CONFIGURE AND START VMS
- name: Clone VMs from templates
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Clone template to VM-LB-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-LB-01
        newid: 200
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-SWARM-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-SWARM-01
        newid: 201
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-SWARM-02
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-SWARM-02
        newid: 202
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-SWARM-03
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-SWARM-03
        newid: 203
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-GIT-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-GIT-01
        newid: 204
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-K3S-01-MASTER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-K3S-01-MASTER
        newid: 205
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-K3S-02-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-K3S-02-WORKER
        newid: 206
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
    - name: Clone template to VM-K3S-03-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        clone: template
        vmid: 1000
        name: VM-K3S-03-WORKER
        newid: 207
        storage: pool-1
        format: raw
        timeout: 300
    - name: Pause for 5 seconds
      ansible.builtin.pause:
        seconds: 5
- name: Update VM configurations with correct hardware assignment and cloud-init variables
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Edit VM-LB-01 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-LB-01
        tags: load-balancer,docker,glusterfs
        cores: 4
        memory: 2048
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.160/24
        update: true
        timeout: 300
    - name: Edit VM-SWARM-01 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-01
        tags: docker,app-server,glusterfs
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.161/24
        update: true
        timeout: 300
    - name: Edit VM-SWARM-02 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-02
        tags: docker,app-server,glusterfs
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.162/24
        update: true
        timeout: 300
    - name: Edit VM-SWARM-03 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-03
        tags: docker,app-server,glusterfs
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.163/24
        update: true
        timeout: 300
    - name: Edit VM-GIT-01 with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-GIT-01
        tags: gitlab,app-server
        cores: 4
        memory: 2048
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.164/24
        update: true
        timeout: 300
    - name: Edit VM-K3S-01-MASTER with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-01-MASTER
        tags: kubernetes,app-server,master
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.165/24
        update: true
        timeout: 300
    - name: Edit VM-K3S-02-WORKER with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-02-WORKER
        tags: kubernetes,app-server,worker
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.166/24
        update: true
        timeout: 300
    - name: Edit VM-K3S-03-WORKER with new cloud-init parameters
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-03-WORKER
        tags: kubernetes,app-server,worker
        cores: 4
        memory: 4096
        citype: nocloud
        ciuser: "{{ username }}"
        cipassword: "{{ password }}"
        sshkeys: "{{ sshkeys }}"
        searchdomains: 'homelab.internal'
        nameservers: 192.168.0.210
        ipconfig: 
          ipconfig0: gw=192.168.0.1,ip=192.168.0.167/24
        update: true
        timeout: 300
- name: Resize VM disks to the required size
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Re-size VM-LB-01 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-LB-01
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-SWARM-01 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-01
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-SWARM-02 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-02
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-SWARM-03 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-03
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-GIT-01 disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-GIT-01
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-K3S-01-MASTER disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-01-MASTER
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-K3S-02-WORKER disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-02-WORKER
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
    - name: Re-size VM-K3S-03-WORKER disk
      community.general.proxmox_disk:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-03-WORKER
        disk: scsi0
        size: 50G
        state: resized
        timeout: 300
- name: Start virtual machines
  hosts: proxmox_servers
  become: true
  tasks:    
    - name: Start VM-LB-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-LB-01
        state: started
        timeout: 300
    - name: Start VM-SWARM-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-01
        state: started
        timeout: 300
    - name: Start VM-SWARM-02
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-02
        state: started
        timeout: 300
    - name: Start VM-SWARM-03
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-SWARM-03
        state: started
        timeout: 300
    - name: Start VM-GIT-01
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-GIT-01
        state: started
        timeout: 300
    - name: Start VM-K3S-01-MASTER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-01-MASTER
        state: started
        timeout: 300
    - name: Start VM-K3S-02-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-02-WORKER
        state: started
        timeout: 300
    - name: Start VM-K3S-03-WORKER
      community.general.proxmox_kvm:
        node: phs-hv-01
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        name: VM-K3S-03-WORKER
        state: started
        timeout: 300
    - name: Pause for 5 minutes
      ansible.builtin.pause:
        seconds: 300
- name: Reboot all VMs to ensure that processes are cleared from install
  hosts: swarm_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Reboot systems [Linux]
      reboot:
        post_reboot_delay: 60
        test_command: uptime

#INITIAL STANDARDISED UPDATES, PACKAGE INSTALLS AND CONFIGURATION
- name: Install updates on newly created VMs
  hosts: swarm_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Preliminary checks [Debian/Ubuntu]
      file:
        path: /var/lib/apt/lists/
        state: directory
        mode: 0755
      when:
        - ansible_facts.os_family == "Debian"
    - name: Install all updates [Debian/Ubuntu]
      apt:
        update_cache: yes
        upgrade: dist
        autoclean: yes
      when:
        - ansible_facts.os_family == "Debian"
    - name: Upgrade all packages and install kernel updates [CentOS/Rocky]
      dnf:
        update_cache: yes
        update_only: yes
        name: '*'
        state: latest
      when:
        - ansible_facts.os_family == "RedHat"
- name: Install base packages and requirements, setup cockpit with plugins and ensure qemu-guest-agent is working
  hosts: swarm_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Install required packages (qemu-guest-agent, cockpit, cifs-utils, nfs-common, python3, pip, neofetch, glusterfs) [Debian/Ubuntu]
      apt:
        name:
        - qemu-guest-agent
        - cockpit
        - cifs-utils
        - nfs-common
        - keyutils
        - snmpd
        - snmp
        - libsnmp-dev
        - software-properties-common
        - apt-transport-https
        - ca-certificates
        - curl
        - python3-pip
        - virtualenv
        - python3-setuptools
        - neofetch
        - glusterfs-server
        - glusterfs-client
        state: present
      when:
        - ansible_facts.os_family == "Debian"
    - name: Install required base-packages (qemu-guest-agent, cifs-utils, nfs-common, python3, pip, neofetch, glusterfs) [CentOS/Rocky]
      dnf:
        name: 
        - qemu-guest-agent
        - cifs-utils
        - nfs-common
        - keyutils
        - epel-release
        - net-snmp
        - net-snmp-utils
        - software-properties-common
        - apt-transport-https
        - ca-certificates
        - curl
        - python3-pip
        - virtualenv
        - python3-setuptools
        - neofetch
        - glusterfs-server
        - glusterfs-client
        - policycoreutils
        state: present
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Install cockpit navigator plugin [Debian/Ubuntu]
      apt:
        deb: https://github.com/45Drives/cockpit-navigator/releases/download/v0.5.10/cockpit-navigator_0.5.10-1focal_all.deb
        state: present
      when:
        - ansible_facts.os_family == "Debian"
    - name: Install cockpit navigator plugin [CentOS/Rocky]
      dnf:
        name: https://github.com/45Drives/cockpit-navigator/releases/download/v0.5.8/cockpit-navigator-0.5.8-2.el8.noarch.rpm
        state: present
        disable_gpg_check: True
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Install cockpit file sharing plugin [Debian/Ubuntu]
      apt:
        deb: https://github.com/45Drives/cockpit-file-sharing/releases/download/v3.3.2/cockpit-file-sharing_3.3.2-1focal_all.deb
        state: present
      when:
        - ansible_facts.os_family == "Debian"
    - name: Install cockpit file sharing plugin [CentOS/Rocky]
      dnf:
        name: https://github.com/45Drives/cockpit-file-sharing/releases/download/v2.4.5/cockpit-file-sharing-2.4.5-1.el8.noarch.rpm
        state: present
        disable_gpg_check: True
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Start qemu-guest-agent service
      systemd:
        name: qemu-guest-agent
        enabled: yes
        state: started
    - name: Create file with NetworkManager config to fix issue with NetworkManager and Cockpit [Debian/Ubuntu]
      copy:
        dest: /etc/NetworkManager/conf.d/10-globally-managed-devices.conf
        content: |
          [keyfile]
          unmanaged-devices=none
      when:
        - ansible_facts.os_family == "Debian"
    - name: Create fake network interface to fix issue with NetworkManager and Cockpit [Debian/Ubuntu]
      ansible.builtin.command: nmcli con add type dummy con-name fake ifname fake0 ip4 1.2.3.4/24 gw4 1.2.3.1
      when:
        - ansible_facts.os_family == "Debian"
    - name: Start cockpit service [Debian/Ubuntu]
      systemd:
        name: cockpit
        enabled: yes
        state: started
        masked: no
      when:
        - ansible_facts.os_family == "Debian"
    - name: Enable cockpit.socket [CentOS/Rocky]
      systemd:
        name: cockpit.socket
        enabled: yes
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Start cockpit [CentOS/Rocky]
      systemd:
        name: cockpit
        state: started
      when:
        - ansible_facts.os_family == "RedHat"
- name: Configure SNMP service and test to ensure it reports correctly
  hosts: swarm_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Install pysnmp for testing SNMP on hosts
      pip:
        name: pysnmp
    - name: Configure SNMP server
      copy: src=/home/abeckett/snmpd.conf dest=/etc/snmp/snmpd.conf owner=root group=root mode=600
    - name: Start and enable SNMPd systemd service.
      systemd:
        name: snmpd
        state: started
        enabled: true
        daemon_reload: true
    - name: Restart SNMPd systemd service if already running.
      systemd:
        name: snmpd
        state: restarted
    - name: Test that SNMP is working
      community.general.snmp_facts:
        host: '{{ inventory_hostname }}'
        version: v2c
        community: BCloudHomelab
      register: response
    - name: Print the results of the test
      debug:
        var: response
- name: Create mountpoint and mount network shares
  hosts: swarm_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Create NAS-01 local directory
      ansible.builtin.file:
        path: /nas-01
        state: directory
    - name: Mount NAS-01 network share
      ansible.posix.mount:
        src: NAS-01.homelab.internal:/volume1/Media
        path: /nas-01
        fstype: nfs
        boot: true
        opts: rw,sync,hard
        state: mounted
    - name: Create STORAGE-01 local directory
      ansible.builtin.file:
        path: /storage-01
        state: directory
    - name: Mount STORAGE-01 network share
      ansible.posix.mount:
        src: STORAGE-01.homelab.internal:/mnt/user/media
        path: /storage-01
        fstype: nfs
        boot: true
        opts: rw,sync,hard
        state: mounted

#SNAPSHOT
- name: Snapshot all VMs after initial configuration before applications and services are installed and started
  hosts: proxmox_servers
  become: true
  tasks:  
    - name: Take a snapshot of VM-LB-01 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 200
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-SWARM-01 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 201
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-SWARM-02 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 202
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-SWARM-03 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 203
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-GIT-01 with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 204
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-K3S-01-MASTER with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 205
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-K3S-02-WORKER with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 206
        snapname: Post-update-early-configuration
        state: present
    - name: Take a snapshot of VM-K3S-03-WORKER with early configuration before role setup
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 207
        snapname: Post-update-early-configuration
        state: present

#GLUSTERFS ON DOCKER SWARM VMS
- name: Start glusterfs/glusterd service and create directory for brick
  hosts: swarm_vms
  become: true
  tasks:
    - name: Start glusterd service
      systemd:
        name: glusterd
        enabled: yes
        state: started
    - name: Create GlusterFS brick-1 directory
      ansible.builtin.file:
        path: /glusterfs/bricks/gv0
        state: directory
- name: Create mountpoint and glusterFS volume and initialize
  hosts: loadbalancer
  become: true
  tasks:
    - name: Create a trusted storage pool
      gluster.gluster.gluster_peer:
        state: present
        nodes:
          - VM-SWARM-01.homelab.internal
          - VM-SWARM-02.homelab.internal
          - VM-SWARM-03.homelab.internal
    - name: Create GlusterFS volume
      gluster.gluster.gluster_volume:
        state: present
        name: gv0
        replicas: 4
        bricks: /glusterfs/bricks/gv0
        cluster:
          - VM-LB-01
          - VM-SWARM-01.homelab.internal
          - VM-SWARM-02.homelab.internal
          - VM-SWARM-03.homelab.internal
        force: true
      run_once: true
    - name: Start GlusterFS volume
      gluster.gluster.gluster_volume:
        state: started
        name: gv0
- name: Create mountpoint and mount glusterFS volume
  hosts: loadbalancer,swarm_manager,swarm_nodes
  become: true
  tasks:
    - name: Create mountpoint for Gluster volume
      ansible.builtin.file:
        path: /mnt/gluster
        state: directory
    - name: Mount Gluster volume
      ansible.posix.mount:
        src: "{{ ansible_host }}:/gv0"
        path: /mnt/gluster
        fstype: glusterfs
        state: mounted       

#DOCKER INSTALL, CONFIGURATION AND SWARM INITIALIZATION
- name: Install docker and docker python module on docker VMs
  hosts: swarm_vms
  become: true
  tasks:    
    - name: Add Docker GPG Key [Debian/Ubuntu]
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
      when:
        - ansible_facts.os_family == "Debian"
    - name: Add Docker Repository [Debian/Ubuntu]
      apt_repository:
        repo: deb https://download.docker.com/linux/ubuntu focal stable
        state: present
      when:
        - ansible_facts.os_family == "Debian"
    - name: Add Docker Repository [CentOS/Rocky]
      shell: |
        dnf install -y yum-utils
        yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
      args:
        warn: no
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Install the latest version of Docker Engine and containerd [Debian/Ubuntu]
      apt:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
        state: latest
        update_cache: true
      when:
        - ansible_facts.os_family == "Debian"
    - name: Install the latest version of Docker Engine and containerd [CentOS/Rocky]
      dnf:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
        state: latest
        update_cache: true
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Add the admin user to the docker group
      user:
        name: "{{ username }}"
        groups: docker
        append: yes
    - name: Install Docker Module for Python
      pip:
        name: docker
    - name: Start and enable Docker systemd service.
      systemd:
        name: docker
        state: started
        enabled: true
        daemon_reload: true
- name: Initialize docker swarm and grab the join token
  hosts: swarm_manager
  become: true
  tasks:
    - name: Check if Swarm has already been Initialized
      shell: docker node ls
      register: swarm_status
      ignore_errors: true
    - name: Initialize Docker Swarm
      docker_swarm:
        state: present
      when: swarm_status.rc != 0
      run_once: true
    - name: Change task-history limit to 0
      shell: docker swarm update --task-history-limit 0
    - name: Get the worker join-token
      shell: docker swarm join-token --quiet worker
      register: worker_token
- name: Add nodes to the docker swarm
  hosts: swarm_nodes
  become: true
  tasks:
    - name: Add Workers to the Swarm
      community.docker.docker_swarm:
        state: join
        advertise_addr: "{{ ansible_host }}"
        join_token: "{{ hostvars[groups['swarm_manager'][0]]['worker_token']['stdout'] }}"
        remote_addrs: [ "{{ hostvars[groups['swarm_manager'][0]]['ansible_default_ipv4']['address'] }}:2377" ]
- name: Deploy docker containers for portainer, NGINX and file manager on the loadbalancer node
  hosts: loadbalancer
  become: true
  tasks:
    - name: Spin-up a Portainer instance for management of Swarm
      docker_container:
        name: Portainer
        image: portainer/portainer-ce
        state: started
        recreate: yes
        restart_policy: always
        published_ports:
          - "8000:8000"
          - "9000:9000"
          - "9443:9443"
        volumes:
          - /var/run/docker.sock:/var/run/docker.sock
          - portainer_data:/data
    - name: Deploy NGINX load-balancer
      community.docker.docker_container:
        name: Load-Balancer
        image: nginx
        state: started
        volumes:
          - /nas-01/Container-configs/loadbalancer/:/etc/nginx/conf.d
        ports:
        - 9003-9020:9003-9020
        - 10000-10020:10000-10020
        - 11000-11020:11000-11020
        - 12000-12020:12000-12020
        - 13000-13020:13000-13020
        - 14000-14020:14000-14020
        - 15000-15020:15000-15020
        restart_policy: always
    - name: Deploy File-Manager for GlusterFS volume
      community.docker.docker_container:
        name: File-Manager
        image: hurlenko/filebrowser
        state: started
        volumes:
          - /nas-01/Container-configs/file-browser-glusterfs:/config
          - /mnt/gluster:/data
        ports:
          - 9002:8080
        env:
          PUID: "1000"
          PGID: "1000"
          TZ: "Europe/London"
        restart_policy: always
- name: Create the portainer agent network and deploy the portainer agent
  hosts: swarm_manager
  become: true
  tasks:
    - name: Create the portainer_agent_network
      docker_network:
        name: portainer_agent_network
        driver: overlay
    - name: Start portainer_agent service
      docker_swarm_service:
        name: portainer_agent
        image: portainer/agent:2.18.2
        networks:
          - portainer_agent_network
        publish:
          - published_port: 9001
            target_port: 9001
            protocol: tcp
        mode: global
        placement:
          constraints:
            - node.platform.os == linux
        mounts:
          - source: //var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
          - source: //var/lib/docker/volumes
            target: /var/lib/docker/volumes
            type: bind

#INSTALL AND CONFIGURE GITLAB
- name: Setup repository and install Gitlab
  hosts: git
  become: true
  tasks:
    - name: Add Gitlab Repository [Debian/Ubuntu]
      shell: curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash
      when:
        - ansible_facts.os_family == "Debian"
    - name: Add Gitlab Repository [CentOS/Rocky]
      shell: curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh | sudo bash
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Pause for 60 seconds
      ansible.builtin.pause:
        seconds: 60
    - name: Install Gitlab [Debian/Ubuntu]
      apt:
        name: gitlab-ee
        state: present
      when:
        - ansible_facts.os_family == "Debian"
    - name: Install Gitlab [CentOS/Rocky]
      dnf:
        name: gitlab-ee
        state: present
      when:
        - ansible_facts.os_family == "RedHat"
    - name: Pause for 3 minutes
      ansible.builtin.pause:
        seconds: 180
    - name: Change External URL in Gitlab config file
      ansible.builtin.lineinfile:
        path: /etc/gitlab/gitlab.rb
        regexp: '^external_url.*$'
        line: 'external_url "http://gitlab.beckettcloud.com"'
    - name: Restart Gitlab to update the changes
      shell: gitlab-ctl restart

#INSTALL AND CONFIGURE K3S
- name: Install K3S on the master node
  hosts: k3s_master
  tasks:
    - name: Install K3S using the official install script to properly pre-configure the service for the master node
      shell: curl -sfL https://get.k3s.io | sh -
    - name: Get the node token and register it
      ansible.builtin.slurp:
        src: /var/lib/rancher/k3s/server/node-token
      register: k3s_token
    - name: Print returned information
      ansible.builtin.debug:
        msg: "{{ k3s_token }}"
- name: Install K3S on the worker nodes
  tasks:
    - name: Install K3S using the official install script to properly pre-configure the service for worker nodes, with the node token supplied by the master
      shell: curl -sfL https://get.k3s.io | K3S_URL="https://{{ hostvars[groups['k3s_master'][0]]['ansible_default_ipv4']['address'] }}:6443" K3S_TOKEN="{{ node-token }}" sh -


#REBOOT
- name: Reboot all VMs to finish updates and ensure all services run correctly
  hosts: swarm_vms,k3s_vms,app_vms
  become: true
  tasks:
    - name: Reboot systems [Linux]
      reboot:
        post_reboot_delay: 60
        test_command: uptime

#SNAPSHOT
- name: Snapshot all VMs after all configuration has been completed and they are ready for active deployment
  hosts: proxmox_servers
  become: true
  tasks:  
    - name: Take a snapshot of VM-LB-01 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 200
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-SWARM-01 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 201
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-SWARM-02 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 202
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-SWARM-03 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 203
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-GIT-01 with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 204
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-K8S-01-MASTER with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 205
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-K8S-02-WORKER with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 206
        snapname: Post-configuration
        state: present
    - name: Take a snapshot of VM-K8S-03-WORKER with final configuration completed
      community.general.proxmox_snap:
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        api_host: phs-hv-01
        vmid: 207
        snapname: Post-configuration
        state: present